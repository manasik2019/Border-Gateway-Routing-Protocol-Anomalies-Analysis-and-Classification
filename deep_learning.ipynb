{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e28c8e82",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553320bd",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a49677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1D CNN Classification\n",
    "\n",
    "#Import the Libraries \n",
    "import math \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Dropout \n",
    "from keras.layers import Dense \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.layers import Flatten \n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "Moscow_blackout = pd.read_csv('Moscow_blackout1.csv')\n",
    "WannaCrypt=pd.read_csv('WannaCrypt1.csv')\n",
    "Nimda=pd.read_csv('Nimda1.csv')\n",
    "Slammer=pd.read_csv('Slammer1.csv')\n",
    "Code_Red_I = pd.read_csv('Code_Red_I1.csv')\n",
    "\n",
    "Datasets = [WannaCrypt, Nimda,Slammer, Moscow_blackout,Code_Red_I ]\n",
    "names = ['WannaCrypt', 'Nimda','Slammer', 'Moscow_blackout','Code_Red_I' ]\n",
    "epochs = [250,300,200,100,300]\n",
    "\n",
    "for i in range(len(Datasets)):\n",
    "    print(\"CNN model for \", names[i])\n",
    "\n",
    "    dataset_x=Datasets[i].drop(['H+M','H','M','S','MED1','MED2','MED3','MED4','MED5','MED6','MED7',\n",
    "                                  'MED8','MED9','MED10','MED11','MAL1','MAL2','MAL3','MAL4','MAL5','MAL6',\n",
    "                                  'MAL7','MAL8','MAL9','Classification'],axis=1)\n",
    "    targets=Datasets[i]['Classification']\n",
    "\n",
    "    dataset_x = dataset_x.values \n",
    "    le = LabelEncoder()\n",
    "    dataset_y = le.fit_transform(targets)\n",
    "\n",
    "    #Split the dataset into training and testing sets (80%: 20%) \n",
    "    x_train,x_test,y_train,y_test=train_test_split(dataset_x, dataset_y, test_size=0.2, random_state=42)\n",
    "\n",
    "    #Scale the all of the data to be values between 0 and 1 \n",
    "    X_scaler = MinMaxScaler()\n",
    "    y_scaler = MinMaxScaler()\n",
    "    scaled_xtrain = X_scaler.fit_transform(x_train)\n",
    "    scaled_xtest = X_scaler.fit_transform(x_test) \n",
    "    #scaled_ytrain = y_scaler.fit_transform(y_train)\n",
    "\n",
    "    #Convert to numpy arrays \n",
    "    scaled_xtrain, scaled_ytrain = np.array(scaled_xtrain), np.array(y_train)\n",
    "\n",
    "    #Reshape the data into 2-D array \n",
    "    scaled_x = np.reshape(scaled_xtrain, (scaled_xtrain.shape[0],scaled_xtrain.shape[1],1)) \n",
    "    scaled_y = np.reshape(scaled_ytrain, (scaled_ytrain.shape[0],1))\n",
    "\n",
    "    model_cnn = Sequential() \n",
    "    model_cnn.add(Conv1D(filters=32, kernel_size=2, activation='relu', input_shape=(scaled_x.shape [1], 1))) \n",
    "    model_cnn.add(MaxPooling1D(pool_size=3))\n",
    "    model_cnn.add(Dropout(0.3))\n",
    "\n",
    "    model_cnn.add(Flatten())\n",
    "    model_cnn.add(Dense(20, activation='softmax')) \n",
    "    model_cnn.add(Dense(1))\n",
    "    model_cnn.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy']) \n",
    "\n",
    "    # fit model \n",
    "    cnn_history = model_cnn.fit(scaled_x, scaled_y, epochs=epochs[i], batch_size=32, verbose=1, validation_split=0.20) \n",
    "\n",
    "    #Convert x_test to a numpy array \n",
    "    x_test = np.array(scaled_xtest)\n",
    "    #Reshape the data into 2-D array \n",
    "    x_test = np.reshape (x_test, (x_test.shape[0],x_test.shape[1],1))\n",
    "    #check predicted values \n",
    "    predictions = model_cnn.predict(x_test) \n",
    "    x_pred = model_cnn.predict(scaled_x) \n",
    " \n",
    "    print(cnn_history.history.keys() )\n",
    "    print(\"\\n\") \n",
    "    # summarize history for loss\n",
    "    plt.plot(cnn_history.history['loss']) \n",
    "    plt.plot(cnn_history.history['val_loss']) \n",
    "    plt.title('Model Loss') \n",
    "    plt.ylabel('Loss') \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylim(0, 0.08)\n",
    "    plt.legend(['train', 'test'], loc='upper right') \n",
    "    plt.show()\n",
    "\n",
    "    # summarize history for accuracy \n",
    "    plt.plot(cnn_history.history['accuracy']) \n",
    "    plt.plot(cnn_history.history['val_accuracy']) \n",
    "    plt.title('Model Accuaracy') \n",
    "    plt.ylabel('Accuracy') \n",
    "    plt.xlabel('Epoch') \n",
    "    plt.ylim(0.05, 0.99)\n",
    "    plt. legend(['train', 'test'], loc='upper left') \n",
    "    plt.show()\n",
    "\n",
    "    test_loss, test_acc = model_cnn.evaluate(scaled_x, scaled_y)\n",
    "    print(\"Accuracy: \", test_acc)\n",
    "\n",
    "    print(\"Training RMSE: \", np.sqrt(mean_squared_error(y_train,x_pred)))\n",
    "    print(\"Testing RMSE: \", np.sqrt(mean_squared_error(y_test, predictions)))\n",
    "    print(\"Accuarcy of CNN model for \" + names[i] + \" is \", test_acc)\n",
    "    total = datetime.now() - start\n",
    "    print(\"Time: \", total, \" minutes\")\n",
    "    print(':\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c794264f",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6aa836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRU Classification\n",
    "\n",
    "#Import the Libraries \n",
    "import math \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import keras\n",
    "from numpy import concatenate\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Dropout \n",
    "from keras.layers import Dense \n",
    "from keras.layers import GRU\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.layers import Flatten \n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "Moscow_blackout = pd.read_csv('Moscow_blackout1.csv')\n",
    "WannaCrypt=pd.read_csv('WannaCrypt1.csv')\n",
    "Nimda=pd.read_csv('Nimda1.csv')\n",
    "Slammer=pd.read_csv('Slammer1.csv')\n",
    "Code_Red_I = pd.read_csv('Code_Red_I1.csv')\n",
    "epochs = [250,300,200,100,300]\n",
    "\n",
    "Datasets = [WannaCrypt, Nimda,Slammer, Moscow_blackout,Code_Red_I ]\n",
    "names = ['WannaCrypt', 'Nimda','Slammer', 'Moscow_blackout','Code_Red_I' ]\n",
    "\n",
    "for i in range(len(Datasets)):\n",
    "    print(\"GRU model for \", names[i])\n",
    "\n",
    "    dataset_x=Datasets[i].drop(['H+M','H','M','S','MED1','MED2','MED3','MED4','MED5','MED6','MED7',\n",
    "                                  'MED8','MED9','MED10','MED11','MAL1','MAL2','MAL3','MAL4','MAL5','MAL6',\n",
    "                                  'MAL7','MAL8','MAL9','Classification'],axis=1)\n",
    "    targets=Datasets[i]['Classification']\n",
    "\n",
    "    dataset_x = dataset_x.values \n",
    "    le = LabelEncoder()\n",
    "    dataset_y = le.fit_transform(targets)\n",
    "\n",
    "    #Split the dataset into training and testing sets (80%: 20%) \n",
    "    x_train,x_test,y_train,y_test=train_test_split(dataset_x, dataset_y, test_size=0.3, random_state=42)\n",
    "\n",
    "    #Scale the all of the data to be values between 0 and 1 \n",
    "    X_scaler = MinMaxScaler()\n",
    "    y_scaler = MinMaxScaler()\n",
    "    scaled_xtrain = X_scaler.fit_transform(x_train)\n",
    "    scaled_xtest = X_scaler.fit_transform(x_test) \n",
    "    #scaled_ytrain = y_scaler.fit_transform(y_train)\n",
    "\n",
    "    #Convert to numpy arrays \n",
    "    scaled_xtrain, scaled_ytrain = np.array(scaled_xtrain), np.array(y_train)\n",
    "\n",
    "    #Reshape the data into 2-D array \n",
    "    scaled_x = np.reshape(scaled_xtrain, (scaled_xtrain.shape[0],scaled_xtrain.shape[1],1)) \n",
    "    scaled_y = np.reshape(scaled_ytrain, (scaled_ytrain.shape[0],1))\n",
    "\n",
    "    model_gru = Sequential() \n",
    "    model_gru.add(GRU(60, return_sequences = False, input_shape=(scaled_x.shape [1], 1))) \n",
    "    model_gru.add(Dropout(0.2))\n",
    "    model_gru.add(Dense(units=1))\n",
    "\n",
    "    #optimizer = RMSprop(learning_rate=0.01)\n",
    "    model_gru.compile(optimizer='SGD', loss='mean_squared_error', metrics=['accuracy']) \n",
    "\n",
    "    # fit model \n",
    "    gru_history = model_gru.fit(scaled_x, scaled_y, epochs=epochs[i], batch_size=32, shuffle=True, verbose=1, \n",
    "                                validation_split=0.20) \n",
    "\n",
    "    #Convert x_test to a numpy array \n",
    "    x_test = np.array(scaled_xtest)\n",
    "\n",
    "    #Reshape the data into 2-D array \n",
    "    x_test = np.reshape (x_test, (x_test.shape[0],x_test.shape[1],1))\n",
    "\n",
    "    #check predicted values \n",
    "    predictions = model_gru.predict(x_test) \n",
    "    x_pred = model_gru.predict(scaled_x) \n",
    "\n",
    "    # List all data in history \n",
    "    print(gru_history.history.keys() )\n",
    "    print(\"\\n\") \n",
    "    # summarize history for accuracy \n",
    "    plt.plot(gru_history.history['loss']) \n",
    "    plt.plot(gru_history.history['val_loss']) \n",
    "    plt.title('Model Loss') \n",
    "    plt.ylabel('Loss') \n",
    "    plt.xlabel('Epoch') \n",
    "    plt.legend(['train', 'test'], loc='upper right') \n",
    "    plt.show()\n",
    "\n",
    "    # summarize history for accuracy \n",
    "    plt.plot(gru_history.history['accuracy']) \n",
    "    plt.plot(gru_history.history['val_accuracy']) \n",
    "    plt.title('Model Accuaracy') \n",
    "    plt.ylabel('Accuracy') \n",
    "    plt.xlabel('Epoch') \n",
    "    plt.ylim(0, 0.8)\n",
    "    plt. legend(['train', 'test'], loc='upper left') \n",
    "    plt.show()\n",
    "\n",
    "    test_loss, test_acc = model_gru.evaluate(x_test, y_test, verbose=2)\n",
    "\n",
    "    print(\"Training RMSE: \", np.sqrt(mean_squared_error(y_train, x_pred)))\n",
    "    print(\"Testing RMSE: \", np.sqrt(mean_squared_error(y_test, predictions)))\n",
    "    print(\"Accuarcy of GRU model for \" + names[i] + \" is \", test_acc)\n",
    "    total = datetime.now() - start\n",
    "    print(\"Time: \", total, \" minutes\")\n",
    "    print(':\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08579373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
